# Real-time Inference Demo for Thermal Object Detection

import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import ipywidgets as widgets
from IPython.display import display, clear_output
import time
import sys
from pathlib import Path

# Add src to path
sys.path.append('..')

# Setup
plt.style.use('seaborn-v0_8-darkgrid')
%matplotlib inline

print("Thermal Object Detection - Real-time Inference Demo")
print("=" * 60)

# 1. Load Sample Thermal Images
print("\n1. LOADING SAMPLE THERMAL IMAGES")
print("-" * 30)

# Create sample images directory
sample_dir = Path('../examples/sample_images')
sample_dir.mkdir(parents=True, exist_ok=True)

# Generate sample thermal images (simulated)
def create_sample_thermal_image(image_type='human', size=(256, 256)):
    """Create simulated thermal images for demo"""
    img = np.zeros(size, dtype=np.float32)
    
    if image_type == 'human':
        # Create human-shaped heat signature
        center_y, center_x = size[0]//2, size[1]//2
        # Head
        cv2.circle(img, (center_x, center_y-30), 15, 180, -1)
        # Body
        cv2.ellipse(img, (center_x, center_y+20), (25, 40), 0, 0, 360, 150, -1)
        # Arms
        cv2.line(img, (center_x-25, center_y), (center_x-50, center_y+20), 140, 8)
        cv2.line(img, (center_x+25, center_y), (center_x+50, center_y+20), 140, 8)
        # Legs
        cv2.line(img, (center_x-10, center_y+60), (center_x-20, center_y+100), 130, 8)
        cv2.line(img, (center_x+10, center_y+60), (center_x+20, center_y+100), 130, 8)
        
    elif image_type == 'animal':
        # Create animal-shaped heat signature
        center_y, center_x = size[0]//2, size[1]//2
        # Body
        cv2.ellipse(img, (center_x, center_y), (40, 25), 0, 0, 360, 160, -1)
        # Head
        cv2.circle(img, (center_x+45, center_y-10), 12, 170, -1)
        # Legs
        for dx in [-25, 0, 25]:
            cv2.line(img, (center_x+dx, center_y+25), (center_x+dx, center_y+60), 140, 6)
        
    elif image_type == 'vehicle':
        # Create vehicle-shaped heat signature
        center_y, center_x = size[0]//2, size[1]//2
        # Main body
        cv2.rectangle(img, (center_x-60, center_y-20), (center_x+60, center_y+20), 200, -1)
        # Hot engine compartment
        cv2.rectangle(img, (center_x-50, center_y-15), (center_x-30, center_y+15), 220, -1)
        # Wheels
        for dx in [-40, 40]:
            cv2.circle(img, (center_x+dx, center_y+25), 12, 180, -1)
            cv2.circle(img, (center_x+dx, center_y-25), 12, 180, -1)
    
    # Add thermal noise
    noise = np.random.randn(*size) * 10
    img = np.clip(img + noise, 0, 255)
    
    return img.astype(np.uint8)

# Create and save sample images
sample_types = ['human', 'animal', 'vehicle']
sample_images = {}

for img_type in sample_types:
    img = create_sample_thermal_image(img_type)
    sample_images[img_type] = img
    
    # Save image
    img_path = sample_dir / f'sample_{img_type}.jpg'
    cv2.imwrite(str(img_path), img)
    print(f"Created sample image: {img_path}")

# 2. Display Sample Images
print("\n\n2. SAMPLE THERMAL IMAGES")
print("-" * 30)

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle('Sample Thermal Images for Detection', fontsize=16, fontweight='bold')

for idx, img_type in enumerate(sample_types):
    img = sample_images[img_type]
    ax = axes[idx]
    im = ax.imshow(img, cmap='hot')
    ax.set_title(f'{img_type.upper()} - Thermal Signature', fontsize=14)
    ax.axis('off')
    
    # Add temperature colorbar
    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

plt.tight_layout()
plt.show()

# 3. Simulated Object Detection
print("\n\n3. OBJECT DETECTION SIMULATION")
print("-" * 30)

class SimulatedDetector:
    """Simulated object detector for demo purposes"""
    
    def __init__(self):
        self.class_names = ['human', 'animal', 'vehicle']
        self.colors = {
            'human': (0, 255, 0),    # Green
            'animal': (255, 165, 0), # Orange
            'vehicle': (0, 0, 255)   # Red
        }
    
    def detect(self, image):
        """Simulate detection on thermal image"""
        # Convert to 3 channels if grayscale
        if len(image.shape) == 2:
            display_img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        else:
            display_img = image.copy()
        
        # Simulate detections based on image content
        detections = []
        
        # Analyze image to guess what's in it
        mean_temp = np.mean(image)
        hot_spots = np.sum(image > 180) / image.size
        
        if mean_temp > 150 and hot_spots > 0.1:
            # Likely human or vehicle (hotter)
            if np.std(image) > 40:  # More variation suggests human
                detections.append({
                    'bbox': [50, 50, 150, 150],
                    'class_name': 'human',
                    'confidence': 0.95,
                    'temperature': mean_temp
                })
            else:  # More uniform suggests vehicle
                detections.append({
                    'bbox': [100, 80, 200, 140],
                    'class_name': 'vehicle',
                    'confidence': 0.87,
                    'temperature': mean_temp
                })
        else:
            # Likely animal (cooler)
            detections.append({
                'bbox': [80, 120, 180, 200],
                'class_name': 'animal',
                'confidence': 0.76,
                'temperature': mean_temp
            })
        
        # Draw detections
        for det in detections:
            x1, y1, x2, y2 = map(int, det['bbox'])
            class_name = det['class_name']
            confidence = det['confidence']
            color = self.colors.get(class_name, (255, 255, 0))
            
            # Draw bounding box
            cv2.rectangle(display_img, (x1, y1), (x2, y2), color, 2)
            
            # Draw label
            label = f"{class_name} {confidence:.1%}"
            (label_width, label_height), baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2
            )
            
            # Label background
            cv2.rectangle(
                display_img,
                (x1, y1 - label_height - baseline - 5),
                (x1 + label_width, y1),
                color,
                -1
            )
            
            # Label text
            cv2.putText(
                display_img,
                label,
                (x1, y1 - baseline - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (255, 255, 255),
                2
            )
            
            # Add temperature info
            temp_text = f"Temp: {det['temperature']:.0f}¬∞"
            cv2.putText(
                display_img,
                temp_text,
                (x1, y2 + 20),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.4,
                color,
                1
            )
        
        return detections, display_img

# Initialize detector
detector = SimulatedDetector()

# Run detection on sample images
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Object Detection Results', fontsize=16, fontweight='bold')

for idx, img_type in enumerate(sample_types):
    img = sample_images[img_type]
    
    # Original image
    ax1 = axes[0, idx]
    im1 = ax1.imshow(img, cmap='hot')
    ax1.set_title(f'Original: {img_type}', fontsize=12)
    ax1.axis('off')
    
    # Detection result
    ax2 = axes[1, idx]
    detections, result_img = detector.detect(img)
    ax2.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))
    ax2.set_title(f'Detections: {len(detections)}', fontsize=12)
    ax2.axis('off')
    
    # Print detection info
    print(f"\n{img_type.upper()} Detection:")
    for det in detections:
        print(f"  - {det['class_name']}: {det['confidence']:.1%} confidence")
        print(f"    BBox: {det['bbox']}, Temp: {det['temperature']:.0f}¬∞")

plt.tight_layout()
plt.show()

# 4. Interactive Demo
print("\n\n4. INTERACTIVE INFERENCE DEMO")
print("-" * 30)

# Create interactive widgets
image_selector = widgets.Dropdown(
    options=[(f'Sample {t}', t) for t in sample_types],
    value='human',
    description='Select Image:',
    style={'description_width': 'initial'}
)

confidence_slider = widgets.FloatSlider(
    value=0.5,
    min=0.1,
    max=1.0,
    step=0.1,
    description='Confidence Threshold:',
    style={'description_width': 'initial'}
)

detect_button = widgets.Button(
    description='Run Detection',
    button_style='success',
    icon='search'
)

output = widgets.Output()

def run_detection(change):
    """Run detection with current settings"""
    with output:
        clear_output(wait=True)
        
        # Get selected image
        img_type = image_selector.value
        img = sample_images[img_type]
        confidence_thresh = confidence_slider.value
        
        print(f"Processing {img_type} image...")
        print(f"Confidence threshold: {confidence_thresh:.1%}")
        
        # Run detection
        start_time = time.time()
        detections, result_img = detector.detect(img)
        inference_time = time.time() - start_time
        
        # Filter by confidence threshold
        filtered_detections = [d for d in detections if d['confidence'] >= confidence_thresh]
        
        # Display results
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Original
        ax1.imshow(img, cmap='hot')
        ax1.set_title('Original Thermal Image', fontsize=14)
        ax1.axis('off')
        
        # Detection result
        ax2.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))
        ax2.set_title(f'Detection Results ({len(filtered_detections)} objects)', fontsize=14)
        ax2.axis('off')
        
        plt.tight_layout()
        plt.show()
        
        # Print statistics
        print(f"\nDetection Results:")
        print(f"  Inference time: {inference_time*1000:.1f} ms")
        print(f"  Total detections: {len(detections)}")
        print(f"  Filtered detections (‚â•{confidence_thresh:.0%}): {len(filtered_detections)}")
        
        if filtered_detections:
            print("\nDetected Objects:")
            for det in filtered_detections:
                print(f"  ‚Ä¢ {det['class_name'].upper()}:")
                print(f"    Confidence: {det['confidence']:.1%}")
                print(f"    Bounding Box: {det['bbox']}")
                print(f"    Temperature: {det['temperature']:.0f}¬∞")
        else:
            print("\n‚ö†Ô∏è  No objects detected above confidence threshold!")

# Connect button click
detect_button.on_click(run_detection)

# Display widgets
print("\nInteractive Detection Demo:")
display(image_selector)
display(confidence_slider)
display(detect_button)
display(output)

# 5. Real-time Simulation
print("\n\n5. REAL-TIME DETECTION SIMULATION")
print("-" * 30)

print("Simulating real-time video feed with object detection...")

# Generate simulated video frames
num_frames = 30
frame_size = (320, 240)
frames = []

print(f"\nGenerating {num_frames} simulated frames...")

for i in range(num_frames):
    # Create frames with moving objects
    frame = np.zeros((frame_size[1], frame_size[0]), dtype=np.uint8)
    
    # Add background noise
    frame = np.random.randint(50, 100, frame.shape, dtype=np.uint8)
    
    # Add moving objects
    if i < 10:
        # Human appears
        cv2.circle(frame, (80 + i*5, 120), 20, 180, -1)
        det_type = 'human'
    elif i < 20:
        # Vehicle appears
        cv2.rectangle(frame, (150, 80 + (i-10)*3), (220, 120 + (i-10)*3), 200, -1)
        det_type = 'vehicle'
    else:
        # Animal appears
        cv2.ellipse(frame, (200 - (i-20)*4, 180), (25, 15), 0, 0, 360, 160, -1)
        det_type = 'animal'
    
    frames.append((frame, det_type))

# Process frames
print("\nProcessing frames with object detection...")

results = []
processing_times = []

for i, (frame, expected_type) in enumerate(frames):
    start_time = time.time()
    detections, result_img = detector.detect(frame)
    end_time = time.time()
    
    processing_times.append(end_time - start_time)
    results.append({
        'frame': i,
        'detections': detections,
        'expected': expected_type,
        'processing_time': end_time - start_time
    })
    
    if (i + 1) % 10 == 0:
        print(f"  Processed {i+1}/{num_frames} frames")

# Analyze results
print("\nReal-time Performance Analysis:")
avg_time = np.mean(processing_times)
fps = 1 / avg_time if avg_time > 0 else 0

print(f"  Average processing time: {avg_time*1000:.1f} ms")
print(f"  Estimated FPS: {fps:.1f}")
print(f"  Total objects detected: {sum(len(r['detections']) for r in results)}")

# Create performance visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Processing time over frames
ax1 = axes[0]
ax1.plot(range(1, num_frames+1), [t*1000 for t in processing_times], 'b-', marker='o')
ax1.axhline(y=np.mean(processing_times)*1000, color='r', linestyle='--', 
           label=f'Mean: {np.mean(processing_times)*1000:.1f}ms')
ax1.set_xlabel('Frame Number')
ax1.set_ylabel('Processing Time (ms)')
ax1.set_title('Inference Time per Frame', fontsize=14, fontweight='bold')
ax1.legend()
ax1.grid(alpha=0.3)

# Detection count over frames
ax2 = axes[1]
detection_counts = [len(r['detections']) for r in results]
ax2.plot(range(1, num_frames+1), detection_counts, 'g-', marker='s')
ax2.set_xlabel('Frame Number')
ax2.set_ylabel('Number of Detections')
ax2.set_title('Object Detection Count', fontsize=14, fontweight='bold')
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# 6. Voice Alert Simulation
print("\n\n6. VOICE ALERT SYSTEM SIMULATION")
print("-" * 30)

print("Simulating voice alert system for detected threats...")

# Simulate voice alerts for critical detections
critical_detections = [
    ('HUMAN DETECTED', 0.95, '50 meters', 'approaching'),
    ('VEHICLE DETECTED', 0.87, '200 meters', 'moving east'),
    ('ANIMAL DETECTED', 0.76, '30 meters', 'stationary')
]

print("\nVoice Alert Simulation:")
print("-" * 40)

for object_type, confidence, distance, status in critical_detections:
    if confidence > 0.7:  # Threshold for alerts
        alert_level = "HIGH" if confidence > 0.9 else "MEDIUM"
        alert_color = "üî¥" if alert_level == "HIGH" else "üü°"
        
        print(f"{alert_color} {alert_level} ALERT: {object_type}")
        print(f"   Confidence: {confidence:.0%}")
        print(f"   Distance: {distance}")
        print(f"   Status: {status}")
        print(f"   Audio: '[BEEP] {object_type} at {distance}, {status}'")
        print()

# 7. Export Results
print("\n\n7. RESULTS EXPORT")
print("-" * 30)

# Create output directory
output_dir = Path('../examples/output_results')
output_dir.mkdir(parents=True, exist_ok=True)

# Save sample results
for img_type in sample_types:
    img = sample_images[img_type]
    detections, result_img = detector.detect(img)
    
    # Save original
    orig_path = output_dir / f'{img_type}_original.jpg'
    cv2.imwrite(str(orig_path), img)
    
    # Save detection result
    result_path = output_dir / f'{img_type}_detection.jpg'
    cv2.imwrite(str(result_path), result_img)
    
    print(f"Saved: {orig_path.name}")
    print(f"Saved: {result_path.name}")

# Save performance data
performance_data = {
    'processing_times': processing_times,
    'detection_counts': detection_counts,
    'avg_fps': fps,
    'sample_detections': critical_detections
}

import json
perf_path = output_dir / 'performance_stats.json'
with open(perf_path, 'w') as f:
    json.dump(performance_data, f, indent=2)

print(f"\nSaved performance data: {perf_path.name}")

# 8. Summary
print("\n\n8. DEMO SUMMARY")
print("-" * 30)

print("\nKey Features Demonstrated:")
print("‚úÖ Thermal image loading and visualization")
print("‚úÖ Object detection with bounding boxes")
print("‚úÖ Confidence-based filtering")
print("‚úÖ Real-time performance simulation")
print("‚úÖ Voice alert system simulation")
print("‚úÖ Results export and visualization")

print("\nNext Steps:")
print("1. Run actual inference: python src/inference/realtime_inference.py")
print("2. Deploy on Raspberry Pi: python src/deployment/raspberry_pi_setup.py")
print("3. Test with real thermal camera footage")

print("\n" + "=" * 60)
print("INFERENCE DEMO COMPLETE")
print("=" * 60)
